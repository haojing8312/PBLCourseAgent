"""
Agent 1 V3: ProjectFoundationAgent - The Strategist
UbD Stage One: ç¡®å®šé¢„æœŸå­¦ä¹ ç»“æœ (G/U/Q/K/Sæ¡†æ¶)
Markdownç‰ˆæœ¬ - ç›´æ¥ç”ŸæˆMarkdownæ–‡æ¡£
"""
import time
from typing import Dict, Any, AsyncGenerator
from pathlib import Path
import logging

from app.core.openai_client import openai_client
from app.core.config import settings

logger = logging.getLogger(__name__)


class ProjectFoundationAgentV3:
    """
    é¡¹ç›®åŸºç¡€å®šä¹‰Agent V3 - Markdownç‰ˆ
    å®ç°UbD Stage One: Desired Results (G/U/Q/K/S)
    ç›´æ¥ç”ŸæˆMarkdownæ ¼å¼æ–‡æ¡£ï¼Œæ— éœ€JSONè§£æ
    """

    def __init__(self):
        self.agent_name = "The Strategist"
        self.timeout = settings.agent1_timeout or 30
        self.phr_version = "v3.0-markdown"

    def _load_phr_prompt(self) -> str:
        """
        ä»PHRæ–‡ä»¶åŠ è½½System Prompt

        Returns:
            str: System promptå†…å®¹
        """
        phr_path = (
            Path(__file__).parent.parent
            / "prompts"
            / "phr"
            / "project_foundation_v3_markdown.md"
        )

        if not phr_path.exists():
            logger.error(f"PHR file not found: {phr_path}")
            raise FileNotFoundError(f"PHR v3-markdown prompt file not found: {phr_path}")

        with open(phr_path, "r", encoding="utf-8") as f:
            content = f.read()

        # æå–System Promptéƒ¨åˆ† (åœ¨```å’Œ```ä¹‹é—´çš„ç¬¬ä¸€ä¸ªä»£ç å—)
        try:
            start_marker = "## System Prompt\n\n```"
            end_marker = "```\n\n---"

            start_idx = content.find(start_marker)
            if start_idx == -1:
                raise ValueError("System Prompt section not found")

            start_idx += len(start_marker)
            end_idx = content.find(end_marker, start_idx)

            if end_idx == -1:
                raise ValueError("End of System Prompt not found")

            system_prompt = content[start_idx:end_idx].strip()
            logger.info(f"Loaded PHR v3-markdown prompt ({len(system_prompt)} chars)")
            return system_prompt

        except Exception as e:
            logger.error(f"Error parsing PHR file: {e}")
            raise ValueError(f"Failed to parse PHR file: {e}")

    def _build_system_prompt(self) -> str:
        """
        æ„å»ºç³»ç»Ÿæç¤ºè¯

        Promptç‰ˆæœ¬: backend/app/prompts/phr/project_foundation_v3_markdown.md
        """
        return self._load_phr_prompt()

    def _build_user_prompt(
        self,
        title: str,
        subject: str = "",
        grade_level: str = "",
        total_class_hours: int = None,
        schedule_description: str = "",
        description: str = "",
    ) -> str:
        """
        æ„å»ºç”¨æˆ·æç¤ºè¯ - Markdownç‰ˆæœ¬

        Args:
            title: è¯¾ç¨‹åç§°
            subject: å­¦ç§‘é¢†åŸŸ
            grade_level: å¹´çº§æ°´å¹³
            total_class_hours: æ€»è¯¾æ—¶æ•°ï¼ˆæŒ‰45åˆ†é’Ÿæ ‡å‡†è¯¾æ—¶ï¼‰
            schedule_description: ä¸Šè¯¾å‘¨æœŸæè¿°
            description: è¯¾ç¨‹ç®€ä»‹
        """
        # æ„å»ºè¯¾ç¨‹æ—¶é•¿ä¿¡æ¯
        duration_info = ""
        if total_class_hours:
            duration_info = f"{total_class_hours}è¯¾æ—¶"
        if schedule_description:
            duration_info += f"ï¼ˆ{schedule_description}ï¼‰" if duration_info else schedule_description

        return f"""# USER INPUT
è¯¾ç¨‹åç§°: {title}
å­¦ç§‘é¢†åŸŸ: {subject or "æœªæŒ‡å®š"}
å¹´çº§æ°´å¹³: {grade_level or "æœªæŒ‡å®š"}
è¯¾ç¨‹æ—¶é•¿: {duration_info or "æœªæŒ‡å®š"}
è¯¾ç¨‹ç®€ä»‹: {description or "æ— "}

è¯·åŸºäºä»¥ä¸Šè¯¾ç¨‹ä¿¡æ¯ï¼Œç”Ÿæˆç¬¦åˆUbDæ¡†æ¶çš„"é˜¶æ®µä¸€ï¼šç¡®å®šé¢„æœŸå­¦ä¹ ç»“æœ"çš„å®Œæ•´Markdownæ–‡æ¡£ã€‚

è¦æ±‚ï¼š
1. ä¸¥æ ¼éµå¾ªä¸Šè¿°æ¨¡æ¿ç»“æ„
2. ç¡®ä¿G/U/Q/K/Sçš„å†…å®¹è´¨é‡ç¬¦åˆæŒ‡å—è¦æ±‚
3. å†…å®¹ç¬¦åˆç›®æ ‡å¹´é¾„æ®µçš„è®¤çŸ¥æ°´å¹³
4. ç›´æ¥è¾“å‡ºMarkdownå†…å®¹ï¼Œä¸è¦ä»»ä½•åŒ…è£¹æˆ–é¢å¤–è¯´æ˜

å¼€å§‹ç”Ÿæˆï¼š"""

    async def generate(
        self,
        title: str,
        subject: str = "",
        grade_level: str = "",
        total_class_hours: int = None,
        schedule_description: str = "",
        description: str = "",
    ) -> Dict[str, Any]:
        """
        ç”ŸæˆStage Oneçš„Markdownæ–‡æ¡£ (G/U/Q/K/S)

        Args:
            title: è¯¾ç¨‹åç§°
            subject: å­¦ç§‘é¢†åŸŸ
            grade_level: å¹´çº§æ°´å¹³
            total_class_hours: æ€»è¯¾æ—¶æ•°ï¼ˆæŒ‰45åˆ†é’Ÿæ ‡å‡†è¯¾æ—¶ï¼‰
            schedule_description: ä¸Šè¯¾å‘¨æœŸæè¿°
            description: è¯¾ç¨‹ç®€ä»‹

        Returns:
            {
                "success": bool,
                "markdown": str,  # Markdownæ–‡æ¡£å­—ç¬¦ä¸²
                "generation_time": float,
                "model": str,
                "error": str (if failed)
            }
        """
        start_time = time.time()

        try:
            logger.info(f"Generating Stage One Markdown for: {title}")

            system_prompt = self._build_system_prompt()
            user_prompt = self._build_user_prompt(
                title, subject, grade_level, total_class_hours, schedule_description, description
            )

            # è°ƒç”¨AI API
            model = settings.agent1_model or settings.openai_model
            response = await openai_client.generate_response(
                prompt=user_prompt,
                system_prompt=system_prompt,
                model=model,
                max_tokens=3000,
                temperature=0.7,
                timeout=self.timeout,
            )

            generation_time = time.time() - start_time

            if not response["success"]:
                logger.error(f"AI generation failed: {response.get('error')}")
                return {
                    "success": False,
                    "error": response.get("error", "AI generation failed"),
                    "generation_time": generation_time,
                    "model": model,
                }

            # ç›´æ¥è¿”å›Markdownå†…å®¹ï¼Œæ— éœ€JSONè§£æ
            markdown_content = response["content"].strip()

            # ç§»é™¤å¯èƒ½çš„markdownä»£ç å—åŒ…è£¹
            if markdown_content.startswith("```markdown"):
                markdown_content = markdown_content[11:].strip()
                if markdown_content.endswith("```"):
                    markdown_content = markdown_content[:-3].strip()
            elif markdown_content.startswith("```"):
                markdown_content = markdown_content[3:].strip()
                if markdown_content.endswith("```"):
                    markdown_content = markdown_content[:-3].strip()

            logger.info(
                f"Stage One Markdown generated successfully in {generation_time:.2f}s"
            )
            logger.info(
                f"Generated markdown length: {len(markdown_content)} characters"
            )

            return {
                "success": True,
                "markdown": markdown_content,
                "generation_time": generation_time,
                "model": model,
            }

        except Exception as e:
            logger.error(f"Agent execution failed: {e}", exc_info=True)
            generation_time = time.time() - start_time
            return {
                "success": False,
                "error": str(e),
                "generation_time": generation_time,
                "model": settings.agent1_model or settings.openai_model,
            }

    async def generate_stream(
        self,
        title: str,
        subject: str = "",
        grade_level: str = "",
        total_class_hours: int = None,
        schedule_description: str = "",
        description: str = "",
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        æµå¼ç”ŸæˆStage Oneçš„Markdownæ–‡æ¡£ (G/U/Q/K/S)

        Args:
            title: è¯¾ç¨‹åç§°
            subject: å­¦ç§‘é¢†åŸŸ
            grade_level: å¹´çº§æ°´å¹³
            total_class_hours: æ€»è¯¾æ—¶æ•°ï¼ˆæŒ‰45åˆ†é’Ÿæ ‡å‡†è¯¾æ—¶ï¼‰
            schedule_description: ä¸Šè¯¾å‘¨æœŸæè¿°
            description: è¯¾ç¨‹ç®€ä»‹

        Yields:
            Dict[str, Any]: æµå¼äº‹ä»¶
            {
                "type": "progress",  # "progress" | "complete" | "error"
                "content": str,      # å½“å‰ç´¯ç§¯çš„markdownå†…å®¹
                "chunk": str,        # æœ¬æ¬¡æ–°å¢çš„æ–‡æœ¬å—ï¼ˆä»…progressäº‹ä»¶ï¼‰
                "progress": float,   # 0.0-1.0 ä¼°ç®—è¿›åº¦
            }
        """
        start_time = time.time()
        accumulated_content = ""
        model = settings.agent1_model or settings.openai_model

        try:
            logger.info(f"Streaming Stage One Markdown for: {title}")

            system_prompt = self._build_system_prompt()
            user_prompt = self._build_user_prompt(
                title, subject, grade_level, total_class_hours, schedule_description, description
            )

            # è°ƒç”¨æµå¼AI API
            chunk_count = 0
            start_stream = time.time()
            logger.info("[STREAM] Agent 1 starting OpenAI streaming...")

            async for chunk in openai_client.generate_response_stream(
                prompt=user_prompt,
                system_prompt=system_prompt,
                model=model,
                max_tokens=3000,
                temperature=0.7,
                timeout=self.timeout,
            ):
                accumulated_content += chunk
                chunk_count += 1
                elapsed = time.time() - start_stream

                # æ¯10ä¸ªchunkè®°å½•ä¸€æ¬¡æ—¥å¿—
                if chunk_count % 10 == 0:
                    logger.info(f"[STREAM] Agent 1 chunk #{chunk_count} @ {elapsed:.2f}s, chars: {len(accumulated_content)}")

                # ğŸ”‘ å…³é”®ï¼šæ¯ä¸ªchunkéƒ½å‘é€è¿›åº¦äº‹ä»¶å¹¶yieldï¼ˆå®æ—¶ï¼‰
                estimated_progress = min(len(accumulated_content) / 2000, 0.99)

                logger.info(f"[STREAM] Agent 1 yielding progress event #{chunk_count}")
                yield {
                    "type": "progress",
                    "content": accumulated_content,
                    "chunk": chunk,
                    "progress": estimated_progress,
                }
                logger.info(f"[STREAM] Agent 1 yielded event #{chunk_count}")

            logger.info(f"[STREAM] Agent 1 finished! Total chunks: {chunk_count}, elapsed: {time.time() - start_stream:.2f}s")

            # æ¸…ç†markdownæ ¼å¼ï¼ˆç§»é™¤å¯èƒ½çš„ä»£ç å—åŒ…è£¹ï¼‰
            final_content = accumulated_content.strip()
            if final_content.startswith("```markdown"):
                final_content = final_content[11:].strip()
                if final_content.endswith("```"):
                    final_content = final_content[:-3].strip()
            elif final_content.startswith("```"):
                final_content = final_content[3:].strip()
                if final_content.endswith("```"):
                    final_content = final_content[:-3].strip()

            generation_time = time.time() - start_time

            logger.info(
                f"Stage One Markdown streaming complete in {generation_time:.2f}s"
            )
            logger.info(
                f"Generated markdown length: {len(final_content)} characters"
            )

            # å‘é€å®Œæˆäº‹ä»¶
            yield {
                "type": "complete",
                "content": final_content,
                "progress": 1.0,
                "generation_time": generation_time,
                "model": model,
            }

        except Exception as e:
            logger.error(f"Agent streaming failed: {e}", exc_info=True)
            generation_time = time.time() - start_time

            yield {
                "type": "error",
                "error": str(e),
                "content": accumulated_content,  # è¿”å›å·²ç”Ÿæˆçš„éƒ¨åˆ†
                "generation_time": generation_time,
                "model": model,
            }


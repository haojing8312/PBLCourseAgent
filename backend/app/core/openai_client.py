"""
OpenAIå®¢æˆ·ç«¯ç®¡ç†
"""
import time
import asyncio
from typing import Any, Dict, AsyncGenerator
from openai import AsyncOpenAI
from app.core.config import settings


class OpenAIClient:
    """OpenAIå®¢æˆ·ç«¯å°è£…ç±»"""

    def __init__(self):
        self.client = AsyncOpenAI(
            api_key=settings.openai_api_key,
            base_url=settings.openai_base_url,
        )

    async def generate_response(
        self,
        prompt: str,
        system_prompt: str = None,
        model: str = None,
        max_tokens: int = None,
        temperature: float = None,
        timeout: int = 60
    ) -> Dict[str, Any]:
        """
        ç”ŸæˆAIå“åº”

        Args:
            prompt: ç”¨æˆ·æç¤ºè¯
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            model: æ¨¡å‹åç§°ï¼Œå¦‚ä¸æŒ‡å®šä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤æ¨¡å‹
            max_tokens: æœ€å¤§ä»¤ç‰Œæ•°ï¼Œå¦‚ä¸æŒ‡å®šä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼
            temperature: æ¸©åº¦å‚æ•°ï¼Œå¦‚ä¸æŒ‡å®šä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

        Returns:
            åŒ…å«å“åº”å†…å®¹å’Œå…ƒæ•°æ®çš„å­—å…¸
        """
        start_time = time.time()

        # ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼
        model = model or settings.openai_model
        max_tokens = max_tokens or settings.openai_max_tokens
        temperature = temperature if temperature is not None else settings.openai_temperature

        try:
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})

            # ä½¿ç”¨asyncio.wait_forè®¾ç½®è¶…æ—¶
            response = await asyncio.wait_for(
                self.client.chat.completions.create(
                    model=model,
                    messages=messages,
                    max_tokens=max_tokens,
                    temperature=temperature,
                ),
                timeout=timeout
            )

            end_time = time.time()
            response_time = end_time - start_time

            return {
                "content": response.choices[0].message.content,
                "response_time": response_time,
                "token_usage": {
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens,
                },
                "model": response.model,
                "success": True,
            }

        except asyncio.TimeoutError:
            return {
                "content": None,
                "response_time": timeout,
                "error": f"Request timeout after {timeout} seconds",
                "success": False,
            }
        except Exception as e:
            end_time = time.time()
            return {
                "content": None,
                "response_time": end_time - start_time,
                "error": str(e),
                "success": False,
            }

    async def generate_response_stream(
        self,
        prompt: str,
        system_prompt: str = None,
        model: str = None,
        max_tokens: int = None,
        temperature: float = None,
        timeout: int = 120
    ) -> AsyncGenerator[str, None]:
        """
        æµå¼ç”ŸæˆAIå“åº”ï¼ˆé€å—yieldæ–‡æœ¬ï¼‰

        Args:
            prompt: ç”¨æˆ·æç¤ºè¯
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            model: æ¨¡å‹åç§°
            max_tokens: æœ€å¤§ä»¤ç‰Œæ•°
            temperature: æ¸©åº¦å‚æ•°
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

        Yields:
            str: æ–‡æœ¬å—

        Raises:
            Exception: ç”Ÿæˆå¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        # ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼
        model = model or settings.openai_model
        max_tokens = max_tokens or settings.openai_max_tokens
        temperature = temperature if temperature is not None else settings.openai_temperature

        try:
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})

            # ä½¿ç”¨asyncio.wait_forè®¾ç½®è¶…æ—¶
            stream = await asyncio.wait_for(
                self.client.chat.completions.create(
                    model=model,
                    messages=messages,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    stream=True,  # ğŸ”‘ å¯ç”¨æµå¼å“åº”
                ),
                timeout=timeout
            )

            # é€å—yieldæ–‡æœ¬
            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content

        except asyncio.TimeoutError:
            raise Exception(f"Request timeout after {timeout} seconds")
        except Exception as e:
            raise Exception(f"Stream generation failed: {str(e)}")


# å…¨å±€å®¢æˆ·ç«¯å®ä¾‹
openai_client = OpenAIClient()
"""
é»„é‡‘æ ‡å‡†æ¡ˆä¾‹æµ‹è¯•
ä½¿ç”¨PRDä¸­å®šä¹‰çš„é»„é‡‘æ ‡å‡†æ¡ˆä¾‹æ¥éªŒè¯ç³»ç»Ÿè¾“å‡ºè´¨é‡
"""
import pytest
import json
from app.core.workflow_service import workflow_service
from app.models.schemas import ProjectInput


class TestGoldenStandard:
    """é»„é‡‘æ ‡å‡†æµ‹è¯•ç±»"""

    @pytest.fixture
    def golden_input(self):
        """é»„é‡‘æ ‡å‡†æ¡ˆä¾‹è¾“å…¥"""
        return ProjectInput(
            course_topic="AIä¹é˜Ÿåˆ¶ä½œäºº",
            course_overview="å­¦ç”Ÿå°†æ‰®æ¼”ä¸€ä¸ªä¹é˜Ÿåˆ¶ä½œäººçš„è§’è‰²ï¼Œä½¿ç”¨AIå·¥å…·ä¸ºä¸€é¦–é¢„è®¾çš„æ­Œè¯åˆ›ä½œæ—‹å¾‹ã€ç¼–æ›²ï¼Œå¹¶åˆ¶ä½œä¸€ä¸ªç®€å•çš„æ­Œè¯MVã€‚",
            age_group="13-15å²",
            duration="2å¤©",
            ai_tools="Suno (AIéŸ³ä¹), Runway (AIè§†é¢‘), Canva (å¹³é¢è®¾è®¡)"
        )

    @pytest.fixture
    def expected_elements(self):
        """é»„é‡‘æ ‡å‡†æ¡ˆä¾‹ä¸­åº”è¯¥åŒ…å«çš„å…³é”®å…ƒç´ """
        return {
            "project_foundation": {
                "driving_question_keywords": ["åˆ¶ä½œäºº", "AIå·¥å…·", "æ­Œæ›²", "è§†å¬"],
                "public_product_keywords": ["æ­Œæ›²", "MV", "å‘å¸ƒä¼š"],
                "required_components": ["æ­Œæ›²", "è§†é¢‘", "å°é¢"],
                "cover_page_elements": ["courseTitle", "tagline", "ageGroup", "duration", "aiTools"]
            },
            "assessment_framework": {
                "min_rubric_dimensions": 3,
                "required_rubric_keywords": ["éŸ³ä¹", "åˆ›æ„", "æŠ€æœ¯"],
                "min_checkpoints": 2,
                "checkpoint_keywords": ["è¯•å¬", "æ£€æŸ¥", "é¢„è§ˆ"]
            },
            "learning_blueprint": {
                "required_prep_materials": ["ç”µè„‘", "è½¯ä»¶", "è®¾å¤‡"],
                "required_skills": ["Suno", "Runway", "Canva"],
                "min_timeline_activities": 5,
                "activity_keywords": ["éŸ³ä¹", "è§†é¢‘", "è®¾è®¡"]
            }
        }

    def calculate_quality_score(self, result_data: dict, expected: dict) -> dict:
        """
        è®¡ç®—è´¨é‡å¾—åˆ†

        Args:
            result_data: ç³»ç»Ÿç”Ÿæˆçš„ç»“æœ
            expected: æœŸæœ›çš„å…ƒç´ 

        Returns:
            åŒ…å«å„ç»´åº¦å¾—åˆ†çš„å­—å…¸
        """
        scores = {
            "project_foundation": 0,
            "assessment_framework": 0,
            "learning_blueprint": 0,
            "overall": 0
        }

        # è¯„ä¼°é¡¹ç›®åŸºç¡€å®šä¹‰
        if "project_foundation" in result_data:
            foundation = result_data["project_foundation"]
            foundation_score = 0

            # æ£€æŸ¥é©±åŠ¨æ€§é—®é¢˜
            if "drivingQuestion" in foundation:
                question = foundation["drivingQuestion"].lower()
                keyword_matches = sum(1 for keyword in expected["project_foundation"]["driving_question_keywords"]
                                    if keyword in question)
                foundation_score += (keyword_matches / len(expected["project_foundation"]["driving_question_keywords"])) * 25

            # æ£€æŸ¥å…¬å¼€æˆæœ
            if "publicProduct" in foundation:
                product = json.dumps(foundation["publicProduct"], ensure_ascii=False).lower()
                keyword_matches = sum(1 for keyword in expected["project_foundation"]["public_product_keywords"]
                                    if keyword in product)
                foundation_score += (keyword_matches / len(expected["project_foundation"]["public_product_keywords"])) * 25

            # æ£€æŸ¥å°é¢é¡µå…ƒç´ 
            if "coverPage" in foundation:
                cover_page = foundation["coverPage"]
                element_matches = sum(1 for element in expected["project_foundation"]["cover_page_elements"]
                                    if element in cover_page)
                foundation_score += (element_matches / len(expected["project_foundation"]["cover_page_elements"])) * 25

            # æ£€æŸ¥å­¦ä¹ ç›®æ ‡
            if "learningObjectives" in foundation:
                objectives = json.dumps(foundation["learningObjectives"], ensure_ascii=False).lower()
                if "ç¡¬æŠ€èƒ½" in objectives or "hardskills" in objectives or "hard skills" in objectives:
                    foundation_score += 12.5
                if "è½¯æŠ€èƒ½" in objectives or "softskills" in objectives or "soft skills" in objectives:
                    foundation_score += 12.5

            scores["project_foundation"] = min(foundation_score, 100)

        # è¯„ä¼°è¯„ä¼°æ¡†æ¶
        if "assessment_framework" in result_data:
            assessment = result_data["assessment_framework"]
            assessment_score = 0

            # æ£€æŸ¥é‡è§„ç»´åº¦æ•°é‡
            if "summativeRubric" in assessment:
                rubric = assessment["summativeRubric"]
                if len(rubric) >= expected["assessment_framework"]["min_rubric_dimensions"]:
                    assessment_score += 25

                # æ£€æŸ¥å…³é”®è¯
                rubric_text = json.dumps(rubric, ensure_ascii=False).lower()
                keyword_matches = sum(1 for keyword in expected["assessment_framework"]["required_rubric_keywords"]
                                    if keyword in rubric_text)
                assessment_score += (keyword_matches / len(expected["assessment_framework"]["required_rubric_keywords"])) * 25

            # æ£€æŸ¥æ£€æŸ¥ç‚¹
            if "formativeCheckpoints" in assessment:
                checkpoints = assessment["formativeCheckpoints"]
                if len(checkpoints) >= expected["assessment_framework"]["min_checkpoints"]:
                    assessment_score += 25

                # æ£€æŸ¥æ£€æŸ¥ç‚¹å…³é”®è¯
                checkpoint_text = json.dumps(checkpoints, ensure_ascii=False).lower()
                keyword_matches = sum(1 for keyword in expected["assessment_framework"]["checkpoint_keywords"]
                                    if keyword in checkpoint_text)
                assessment_score += (keyword_matches / len(expected["assessment_framework"]["checkpoint_keywords"])) * 25

            scores["assessment_framework"] = min(assessment_score, 100)

        # è¯„ä¼°å­¦ä¹ è“å›¾
        if "learning_blueprint" in result_data:
            blueprint = result_data["learning_blueprint"]
            blueprint_score = 0

            # æ£€æŸ¥æ•™å¸ˆå‡†å¤‡
            if "teacherPrep" in blueprint:
                prep = blueprint["teacherPrep"]
                if "materialList" in prep and len(prep["materialList"]) >= 3:
                    blueprint_score += 20

                if "skillPrerequisites" in prep and len(prep["skillPrerequisites"]) >= 2:
                    blueprint_score += 20

                # æ£€æŸ¥å…³é”®æŠ€èƒ½
                prep_text = json.dumps(prep, ensure_ascii=False).lower()
                skill_matches = sum(1 for skill in expected["learning_blueprint"]["required_skills"]
                                  if skill.lower() in prep_text)
                blueprint_score += (skill_matches / len(expected["learning_blueprint"]["required_skills"])) * 20

            # æ£€æŸ¥æ—¶é—´çº¿
            if "timeline" in blueprint:
                timeline = blueprint["timeline"]
                if len(timeline) >= expected["learning_blueprint"]["min_timeline_activities"]:
                    blueprint_score += 20

                # æ£€æŸ¥æ´»åŠ¨å…³é”®è¯
                timeline_text = json.dumps(timeline, ensure_ascii=False).lower()
                keyword_matches = sum(1 for keyword in expected["learning_blueprint"]["activity_keywords"]
                                    if keyword in timeline_text)
                blueprint_score += (keyword_matches / len(expected["learning_blueprint"]["activity_keywords"])) * 20

            scores["learning_blueprint"] = min(blueprint_score, 100)

        # è®¡ç®—æ€»åˆ†
        scores["overall"] = (scores["project_foundation"] +
                           scores["assessment_framework"] +
                           scores["learning_blueprint"]) / 3

        return scores

    @pytest.mark.asyncio
    @pytest.mark.slow  # æ ‡è®°ä¸ºæ…¢é€Ÿæµ‹è¯•ï¼Œéœ€è¦çœŸå®APIè°ƒç”¨
    async def test_golden_standard_quality(self, golden_input, expected_elements):
        """
        æµ‹è¯•é»„é‡‘æ ‡å‡†æ¡ˆä¾‹çš„è´¨é‡
        æ³¨æ„ï¼šè¿™ä¸ªæµ‹è¯•éœ€è¦çœŸå®çš„OpenAI APIå¯†é’¥æ‰èƒ½è¿è¡Œ
        """
        try:
            # æ‰§è¡Œå®Œæ•´å·¥ä½œæµç¨‹
            result = await workflow_service.execute_full_workflow(golden_input)

            # éªŒè¯åŸºæœ¬æˆåŠŸ
            assert result["success"] is True, f"å·¥ä½œæµç¨‹æ‰§è¡Œå¤±è´¥: {result.get('message', 'Unknown error')}"
            assert "data" in result

            # éªŒè¯æ•°æ®ç»“æ„å®Œæ•´æ€§
            data = result["data"]
            assert "project_foundation" in data
            assert "assessment_framework" in data
            assert "learning_blueprint" in data
            assert "metadata" in data

            # è®¡ç®—è´¨é‡å¾—åˆ†
            quality_scores = self.calculate_quality_score(data, expected_elements)

            print(f"\nğŸ“Š é»„é‡‘æ ‡å‡†æ¡ˆä¾‹è´¨é‡è¯„ä¼°ç»“æœ:")
            print(f"   é¡¹ç›®åŸºç¡€å®šä¹‰: {quality_scores['project_foundation']:.1f}/100")
            print(f"   è¯„ä¼°æ¡†æ¶è®¾è®¡: {quality_scores['assessment_framework']:.1f}/100")
            print(f"   å­¦ä¹ è“å›¾ç”Ÿæˆ: {quality_scores['learning_blueprint']:.1f}/100")
            print(f"   æ€»ä½“è´¨é‡å¾—åˆ†: {quality_scores['overall']:.1f}/100")

            # éªŒè¯æ€§èƒ½æŒ‡æ ‡
            metadata = data["metadata"]
            print(f"\nâ±ï¸  æ€§èƒ½æŒ‡æ ‡:")
            print(f"   æ€»è€—æ—¶: {metadata['total_time']:.2f}ç§’")
            print(f"   Agent 1: {metadata['agent_times'].get('agent1', 0):.2f}ç§’")
            print(f"   Agent 2: {metadata['agent_times'].get('agent2', 0):.2f}ç§’")
            print(f"   Agent 3: {metadata['agent_times'].get('agent3', 0):.2f}ç§’")

            # æ–­è¨€è´¨é‡æ ‡å‡†
            assert quality_scores["overall"] >= 80.0, f"è´¨é‡å¾—åˆ† {quality_scores['overall']:.1f} ä½äº80%çš„ç›®æ ‡é˜ˆå€¼"

            # æ–­è¨€æ€§èƒ½æ ‡å‡†
            assert metadata["total_time"] < 90, f"æ€»è€—æ—¶ {metadata['total_time']:.2f}ç§’ è¶…è¿‡90ç§’çš„ç›®æ ‡"

            print(f"\nâœ… é»„é‡‘æ ‡å‡†æ¡ˆä¾‹æµ‹è¯•é€šè¿‡ï¼")

        except Exception as e:
            if "API key" in str(e) or "openai" in str(e).lower():
                pytest.skip(f"è·³è¿‡é»„é‡‘æ ‡å‡†æµ‹è¯•ï¼Œéœ€è¦OpenAI APIå¯†é’¥: {str(e)}")
            else:
                raise e

    def test_quality_score_calculation(self, expected_elements):
        """æµ‹è¯•è´¨é‡å¾—åˆ†è®¡ç®—é€»è¾‘"""
        # æ¨¡æ‹Ÿä¸€ä¸ªå®Œç¾çš„ç»“æœ
        mock_result = {
            "project_foundation": {
                "drivingQuestion": "ä½œä¸ºä¸€åæ–°ç”Ÿä»£çš„éŸ³ä¹åˆ¶ä½œäººï¼Œæˆ‘ä»¬å¦‚ä½•ä»…å‡­AIå·¥å…·å°±èƒ½åˆ›ä½œå‡ºä¸€é¦–èƒ½è§¦åŠ¨äººå¿ƒçš„æ­Œæ›²ï¼Ÿ",
                "publicProduct": {
                    "description": "ä¸€é¦–å®Œæ•´çš„æ­Œæ›²åŠå…¶MVï¼Œå°†ä¸¾åŠå‘å¸ƒä¼šå±•ç¤º"
                },
                "learningObjectives": {
                    "hardSkills": ["éŸ³ä¹æŠ€èƒ½"],
                    "softSkills": ["åˆ›æ„èƒ½åŠ›"]
                },
                "coverPage": {
                    "courseTitle": "AIä¹é˜Ÿåˆ¶ä½œäºº",
                    "tagline": "æ ‡è¯­",
                    "ageGroup": "13-15å²",
                    "duration": "2å¤©",
                    "aiTools": "AIå·¥å…·"
                }
            },
            "assessment_framework": {
                "summativeRubric": [
                    {"dimension": "éŸ³ä¹åˆ›æ„æŠ€æœ¯èƒ½åŠ›"},
                    {"dimension": "ç¬¬äºŒç»´åº¦"},
                    {"dimension": "ç¬¬ä¸‰ç»´åº¦"}
                ],
                "formativeCheckpoints": [
                    {"name": "éŸ³ä¹è¯•å¬æ£€æŸ¥"},
                    {"name": "è§†é¢‘é¢„è§ˆæ£€æŸ¥"}
                ]
            },
            "learning_blueprint": {
                "teacherPrep": {
                    "materialList": ["ç”µè„‘", "è½¯ä»¶", "è®¾å¤‡", "å…¶ä»–"],
                    "skillPrerequisites": ["SunoæŠ€èƒ½", "RunwayæŠ€èƒ½", "CanvaæŠ€èƒ½"]
                },
                "timeline": [
                    {"activityTitle": "éŸ³ä¹åˆ›ä½œ"},
                    {"activityTitle": "è§†é¢‘åˆ¶ä½œ"},
                    {"activityTitle": "è®¾è®¡å·¥ä½œ"},
                    {"activityTitle": "æ´»åŠ¨4"},
                    {"activityTitle": "æ´»åŠ¨5"}
                ]
            }
        }

        scores = self.calculate_quality_score(mock_result, expected_elements)

        # éªŒè¯å¾—åˆ†èŒƒå›´
        for component, score in scores.items():
            assert 0 <= score <= 100, f"{component} å¾—åˆ† {score} ä¸åœ¨æœ‰æ•ˆèŒƒå›´å†…"

        # å¯¹äºå®Œç¾åŒ¹é…çš„æ¡ˆä¾‹ï¼Œå¾—åˆ†åº”è¯¥å¾ˆé«˜
        assert scores["overall"] >= 80, f"å®Œç¾æ¡ˆä¾‹çš„æ€»å¾—åˆ† {scores['overall']} åº”è¯¥é«˜äº80%"